---
title: "Clustering"
author: "Rafael A. Irizarry"
date: "`r lubridate::today()`"
output:
  ioslides_presentation:
    fig_caption: no
    fig_height: 5
    fig_width: 7
    out_width: "70%"
  beamer_presentation: default
  slidy_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(dslabs)
library(gridExtra)
library(ggthemes)
ds_theme_set()
options(digits = 3)
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  cache = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.asp = 0.618,  # 1 / phi
  fig.show = "hold"
)

img_path <- "img"
```


## Clustering

- The algorithms we have described up to now are examples of a general approach referred to as _supervised_ machine learning.

- The name comes from the fact that we use the outcomes in a training set to _supervise_ the creation of our prediction algorithm.

- There is another subset of machine learning referred to as _unsupervised_.

- In this subset we do not necessarily know the outcomes and instead are interested in discovering groups.



## Clustering

- These algorithms are also referred to as _clustering_ algorithms since predictors are used to define _clusters_.

- In the two examples we have shown here, clustering would not be very useful.

- In the first example, if we are simply given the heights we will not be able to discover two groups, males and females, because the intersection is large.

## Clustering

- In the second example, we can see from plotting the predictors that discovering the two digits, two and seven, will be challenging:

```{r mnist-27-unsupervised, message=FALSE, warning=FALSE, eval=FALSE} 
library(tidyverse) 
library(dslabs) 
data("mnist_27") 
mnist_27$train |> qplot(x_1, x_2, data = _) 
``` 


## Clustering

```{r mnist-27-unsupervised-run, message=FALSE, warning=FALSE, echo=FALSE} 
library(tidyverse) 
library(dslabs) 
data("mnist_27") 
mnist_27$train |> qplot(x_1, x_2, data = _) 
``` 


## Clustering

- However, there are applications in which unsupervised learning can be a powerful technique, in particular as an exploratory tool.

- A first step in any clustering algorithm is defining a distance between observations or groups of observations.

- Then we need to decide how to join observations into clusters.

- There are many algorithms for doing this.

## Clustering

- Here we introduce two as examples: hierarchical and k-means.


- We will construct a simple example based on movie ratings.

- Here we quickly construct a matrix `x` that has ratings for the 50 movies with the most ratings.



## Clustering

```{r} 
data("movielens") 
top <- movielens |> 
  group_by(movieId) |> 
  summarize(n=n(), title = first(title)) |> 
  top_n(50, n) |> 
  pull(movieId) 
x <- movielens |>  
  filter(movieId %in% top) |> 
  group_by(userId) |> 
  filter(n() >= 25) |> 
  ungroup() |>  
  select(title, userId, rating) |> 
  spread(userId, rating) 
row_names <- str_remove(x$title, ": Episode") |> str_trunc(20) 
x <- x[,-1] |> as.matrix() 
x <- sweep(x, 2, colMeans(x, na.rm = TRUE)) 
x <- sweep(x, 1, rowMeans(x, na.rm = TRUE)) 
rownames(x) <- row_names 
``` 



## Clustering

- We want to use these data to find out if there are clusters of movies based on the ratings from  `r ncol(x)` movie raters.

- A first step is to find the distance between each pair of movies using the `dist` function:

```{r} 
d <- dist(x) 
``` 



## Hierarchical clustering

- With the distance between each pair of movies computed, we need an algorithm to define groups from these.

- Hierarchical clustering starts by defining each observation as a separate group, then the two closest groups are joined into a group iteratively until there is just one group including all the observations.

- The `hclust` function implements this algorithm and it takes a distance as input.

```{r} 
h <- hclust(d) 
``` 

- We can see the resulting groups using a _dendrogram_.



## Hierarchical clustering

```{r, eval=FALSE} 
plot(h, cex = 0.65, main = "", xlab = "") 
``` 


## Hierarchical clustering

```{r dendrogram, out.width="100%", fig.width = 8, fig.height = 3, echo=FALSE} 
rafalib::mypar() 
plot(h, cex = 0.65, main = "", xlab = "") 
``` 


## Hierarchical clustering

- This graph gives us an approximation between the distance between any two movies.

- To find this distance we find the first location, from top to bottom, where these movies split into two different groups.

- The height of this location is the distance between these two groups.

- So, for example, the distance between the three _Star Wars_ movies is 8 or less, while the distance between _Raiders of the Lost of Ark_ and _Silence of the Lambs_ is about 17.



## Hierarchical clustering

- To generate actual groups we can do one of two things: 1) decide on a minimum distance needed for observations to be in the same group or 2) decide on the number of groups you want and then find the minimum distance that achieves this.

- The function `cutree` can be applied to the output of `hclust` to perform either of these two operations and generate groups.

```{r} 
groups <- cutree(h, k = 10) 
``` 

## Hierarchical clustering

- Note that the clustering provides some insights into types of movies.

- Group 4 appears to be award winning and popular movies:

```{r} 
names(groups)[groups==4] 
``` 

## Hierarchical clustering

- And group 9 appears to be nerd movies:

```{r} 
names(groups)[groups==9] 
``` 

## Hierarchical clustering

- We can change the size of the group by either making `k` larger or `h` smaller.

- We can also explore the data to see if there are clusters of movie raters.

## Hierarchical clustering


```{r} 
h_2 <- dist(t(x)) |> hclust() 
``` 

```{r dendrogram-2, , out.width="100%", fig.height=4, eval=FALSE} 
plot(h_2, cex = 0.35) 
``` 

```{r dendrogram-2-run, , out.width="100%", fig.height=4, echo=FALSE} 
plot(h_2, cex = 0.35) 
``` 


## k-means

- To use the k-means clustering algorithm we have to pre-define $k$, the number of clusters we want to define.

- The k-means algorithm is iterative.

- The first step is to define $k$ centers.

- Then each observation is assigned to the cluster with the closest center to that observation.

- In a second step the centers are redefined using the observation in each cluster: the column means are used to define a _centroid_.



## k-means

- We repeat these two steps until the centers converge.

- The `kmeans` function included in R-base does not handle NAs.

- For illustrative purposes we will fill out the NAs with 0s.

- In general, the choice of how to fill in missing data, or if one should do it at all, should be made with care.

```{r} 
x_0 <- x 
x_0[is.na(x_0)] <- 0 
k <- kmeans(x_0, centers = 10) 
``` 




## k-means

- The cluster assignments are in the `cluster` component:

```{r} 
groups <- k$cluster 
``` 

- Note that because the first center is chosen at random, the final clusters are random.

- We impose some stability by repeating the entire function several times and averaging the results.

- The number of random starting values to use can be assigned through the `nstart` argument.

```{r} 
k <- kmeans(x_0, centers = 10, nstart = 25) 
``` 



## Heatmaps

- A powerful visualization tool for discovering clusters or patterns in your data is the heatmap.

- The idea is simple: plot an image of your data matrix with colors used as the visual cue and both the columns and rows ordered according to the results of a clustering algorithm.

- We will demonstrate this with the `tissue_gene_expression` dataset.

- We will scale the rows of the gene expression matrix.




## Heatmaps

- The first step is compute:

```{r} 
data("tissue_gene_expression") 
x <- sweep(tissue_gene_expression$x, 2, colMeans(tissue_gene_expression$x)) 
h_1 <- hclust(dist(x)) 
h_2 <- hclust(dist(t(x))) 
``` 

- Now we can use the results of this clustering to order the rows and columns.

```{r heatmap, out.width="100%", fig.height=7, eval=FALSE} 
image(x[h_1$order, h_2$order]) 
``` 

## Heatmaps

- But there is `heatmap` function that does it for us:

```{r heatmap-2, out.width="100%", fig.height=7, eval=FALSE} 
heatmap(x, col = RColorBrewer::brewer.pal(11, "Spectral")) 
``` 

- We do not show the results of the heatmap function because there are too many features for the plot to be useful.




- We will therefore filter some columns and remake the plots.



## Filtering features

- If the information about clusters is included in just a few features, including all the features can add enough noise that detecting clusters becomes challenging.

- One simple approach to try to remove features with no information is to only include those with high variance.

- In the movie example, a user with low variance in their ratings is not really informative: all the movies seem about the same to them.



## Filtering features

- Here is an example of how we can include only the features with high variance.


```{r[]} 
library(matrixStats) 
sds <- colSds(x, na.rm = TRUE) 
o <- order(sds, decreasing = TRUE)[1:25] 
``` 

## Filtering features

```{r heatmap-3, out.width="100%", fig.height=5, fig.width=6, message=FALSE, warning=FALSE} 
heatmap(x[,o], col = RColorBrewer::brewer.pal(11, "Spectral")) 
``` 
